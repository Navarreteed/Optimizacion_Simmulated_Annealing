{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporte de Proyecto Final - Simulated Annealing (SA)\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "* Brandon Francisco Hernández Troncoso\n",
    "* José Eduardo Gutiérrez Navarrete\n",
    "* Miguel Ángel Castañeda Martínez\n",
    "\n",
    "## Objetivos:\n",
    "* Entender el funcionamiento de SA \n",
    "* Replicar SA en una función de Python\n",
    "* Probar SA en funciones conocidas (Parábola, Ackley y Coseno)\n",
    "* Utilizar la funcion SA para resolver el problema del agente viajero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del Algoritmo\n",
    "*Simulated annealing*, o recocido simulado en español, es un algoritmo metaheurístico propuesto por Kirkpatrick en la revista *Science* en 1983 (Kirkpatrick et al., 1983). En este artículo se muestra que el algoritmo puede resolver problemas de optimización tomando como base el conocimiento de la estadística mecánica y optimización combinatoria multivariada. Al igual que otros métodos metaheurísticos, SA está basado en un fenómeno causado por el proceso termodinámico de calentamiento y enfriamiento del recocido del metal, el cual permite deformar los materiales a temperaturas muy altas, pero conforme la temperatura se disminuye, el material resulta menos maleable y se deforma con menor facilidad (Press & Al, 2007). \n",
    "\n",
    "El algoritmo puede ser considerado como un punto medio entre el algoritmo de colinas ascendentes, que suele estancarse en mínimos locales, y algún otro algoritmo que se mueva de manera completamente aleatoria entre un conjunto de sucesores, el cual sería muy completo pero sumamente ineficiente. La intuición detrás de SA es que las primeras aproximaciones son mas descuidadas, es decir, que la probabilidad de aceptación de un movimiento \"malo\" es mas alta. Sin emabrgo, conforme el algoritmo se acerca al mínimo global, el algoritmo realiza movimientos más finos, lo que permite que la probabilidad de aceptación de un movimiento \"malo\" sea cada vez más baja, provocando de esta manera que se escojan mejores movimientos conforme se realizan las iteraciones (Liang, 2020). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación del algoritmo\n",
    "\n",
    "Como se mencionó anteriormente, SA toma su inspiración de la metalurgia, donde la temperatura es usada para controlar el grado de estocasticidad durante una búsqueda aleatoria. La temperatura se inicializa en un valor alto, permitiéndole al algoritmo poderse mover libremente sobre el espacio de búsqueda, en este caso la función a optimizar. Con lo anterior en mente, se tiene la esperanza de poder encontrar una región con el mejor mínimo local, el cuál se espera que sea equivalente al mínimo global. Sin embargo, para poder dicha región, también es necesario que la temperatura sea disminuida lentamente conforme se itera en el algoritmo, lo que permite reducir la estocasticidad y forza la búsqueda a converger a algún punto mínimo (Kochnderfer & Wheeler, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada iteración, un transición candidata de $x$ a $x'$ es muestrada de una distribución $T$ y es aceptada con probabilidad:\n",
    "\n",
    "* $1$ si $\\Delta y \\leq 0$ \n",
    "* $exp(\\frac{-\\Delta y}{t})$ si $\\Delta y > 0$\n",
    "\n",
    "donde $\\Delta y=f(x')-f(x)$ es la diferencia del objetivo y $t$ es la temperatura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que vuelve a este algoritmo un método tan flexible es la aceptación de las probabilidades basado en el *crietrio Metropolis*, el cual ayuda al algoritmo a escapar de minimos locales cuando la temperatura es alta. Como es de esperarse, se requiere del parámetro que representa a la **temperatura** $t$, el cual controla la aceptación de la probabilidad, además de requerir de un **itinerario de recocido** que es usado para disminuir lentamente la temperatura conforme el algoritmo realiza las iteraciones, el cual también ayuda a lograr una convergencia con cada valor de temperatura disminuído (Kochnderfer & Wheeler, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de las restricciones que tiene este algoritmo destaca la velocidad con la que la temperatura debe ser disminuida, ya que si la temperatura disminuye su valor muy rápido, entonces la busqueda puede no llegar a converger a un mínino global, mientras que si la temperatura disminuye a un ritmo lento, entonces el algoritmo puede tomar mucho tiempo en converger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varios tipos de itinerario de recocido (Kochnderfer & Wheeler, 2019), donde los más utilizados son:\n",
    "\n",
    "* **logarítmico**: $t^{(k)}=t^{(1)}\\frac{ln(2)}{ln(k+1)}$, \n",
    "* **exponencial**: $t^{(k+1)}=\\gamma t^{(k)}$, $\\gamma\\in (0,1)$, \n",
    "* **recocido rápido**: $t^{(k)} = \\frac{t^{(1)}}{k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura 1 - itinerarios de recocido \n",
    "<br> ![image](imagenes/Xnip2022-12-10_01-03-02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del problema de optimización a resolver\n",
    "\n",
    "La intención de este trabajo al usar SA es demostrar su funcionamiento y eficacia ante problemas de optimización, donde buscamos encontrar puntos mínimos globales según las restricciones con las que se cuente. Para cumplir con este objetivo, se buscó poner a prubeba el método de SA para varias funciones definidas, las cuales se caracterizan por tener al menos un punto mínimo. Así mismo, se replicó la resolución del *problema del agente viajero*, el cual se explicará más tarde en este trabajo.\n",
    "\n",
    "## ¿Qué tipo de problema de optimización es?\n",
    "\n",
    "El problema que buscamos resolver es poder encontrar mínimos globales en funciones que se caracterizan por contener múltiples mínimos locales. De esta manera, buscamos aprovechar la ventaja de la **optimización estocástica** que caracteriza a SA, con la cual utilizamos aleatorización estratégica para explorar todo el espacio de la función de interés. Esta misma aleatoriedad que caracteriza a los métodos de optimización estocástica es lo que les ayuda a no quedarse estancados en puntos óptimos locales, brindando la capacidad de recorrer un mayor espacio de la función en búsqueda de los puntos óptimos globales con base en probabilidades de aceptación (Kochnderfer & Wheeler, 2019).\n",
    "\n",
    "## Métodos alternativos\n",
    "\n",
    "Dentro de la optimización estocástica existen varios métodos que nos pudieran ayudar a encontrar puntos mínimos globales, tales como *Gradiente descendente estocástico*, *Búsqueda directa adaptativa de malla*, o *entropía cruzada* (Kochnderfer & Wheeler, 2019). Cada uno de ellos posee ventajas que los hacen una opción viable, además de que son metodos que han sido estudiados por la comunidad y de los cuales es sencillo encontrar información. Sin embargo, después de una comparación entre los distintos tipos de algoritmos y un análisis de su respectiva implementación, se decidió que SA podría ser más intuitivo de entender, además de que el origen del algoritmo es del área de ingeniería, área de la que los integrantes de este equipo provienen, por lo que se tomó la decisión de replicar SA con base en la modelación matemática de un fenómeno de estudio de nuestra área. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer enfoque que se tomó para replicar el método SA fue la definición de funciones matemáticas univariadas, es decir, funciones que para cada valor de $x$ se tiene únicamente un valor resultado $y$. Se tomó la decisión de trabajar con este tipo de funciones en primera instancia debido a su fácil visualización, además de que nos permite mostrar el funcionamiento de SA con un previo conocimiento de las funciones y sus respectivos puntos globales a encontrar. \n",
    "\n",
    "La primer función considerada para este análisis fue una __parábola__ definida por la ecuación __$y=(x-1)$__, donde su punto mínimo global está en $x=1$. Si bien esta función no tiene múltiples puntos mínimos, resulta ser un muy buen primer acercamiento para poner a prueba nuestro algoritmo programado, además de que la función es sencilla de entender y la visualización del funcionamiento del algoritmo puede hacerse explícita al ser una función bien conocida.\n",
    "\n",
    "Un fragmento de la gráfica de la función se muestra a continuación: \n",
    "\n",
    "<br>![image](imagenes/parabola_inicial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra siguiente función considerada fue resultado de __la suma de dos funciones individuales__ bien conocidas, la primera de ellas $g(x) = \\lvert x \\rvert$, y la segunda $h(x) = cos(x)$. La función que involucra el valor absoluto registra un único mínimo global en $x=0$, mientras que la función coseno registra múltiples puntos mínimos en $x = 2k\\pi + \\pi$, con $k \\in \\mathbb{Z}$ . Por lo tanto, la segunda función está definida por la ecuación $y = \\lvert x \\rvert + cos(x)$, la cual involucra múltiples puntos mínimos locales y un punto mínimo global en $x = 0$\n",
    "\n",
    "Un fragmento de la gráfica de la función se muestra a continuación:\n",
    "\n",
    "<br>![image](imagenes/cos_inicial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última función definida fue la __función Ackley__, la cual se caracteriza por tener múltiples puntos mínimos y máximos a lo largo de todo su dominio. Esta función es típicamente usada para poner a prueba algoritmos de optimización, ya que es fácil que un algoritmo pueda quedarse *atrapado* debido a la gran cantidad de puntos mínimos que la definen, además de la gran cercanía entre cada uno de ellos. (Surjanovic & Bingham, 2013).\n",
    "\n",
    "La definición de la __función Ackley__ está dada por la ecuación:\n",
    "\n",
    "$$\n",
    "y = -a*exp\\left(-b\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_{i}^{2}}\\right) - exp\\left(\\frac{1}{d}\\sum_{i=1}^{d}cos\\left(c x_{i}\\right)\\right) + a + exp(1)\n",
    "$$\n",
    "\n",
    "donde $d$ equivale al número de variables independientes, y $a$, $b$ y $c$ son valores modificables. De acuerdo con nuestras referencias, los valores recomendados para cada uno de ellos son $a=20$, $b=0.2$ y $c=2\\pi$, por lo que en este trabajo se decidió por tomar en cuenta dichas recomendaciones para implementar la función Ackley. \n",
    "\n",
    "Un fragmento de la gráfica de la función se muestra a continuación:\n",
    "\n",
    "\n",
    "<br>![image](imagenes/ackley_inicial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación en Python de SA para funciones predefinidas\n",
    "\n",
    "Las siguientes gráficas muestran el camino que tomó el algoritmo de SA para lograr la aproximación al punto mínimo global.\n",
    "\n",
    "* Parábola $f(x) = (x-1)^2$\n",
    "<br>![image](imagenes/Parabola.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coseno $f(x) = \\lvert x \\rvert + cos(x)$\n",
    "<br>![image](imagenes/Coseno.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Función Ackley con parámetros $a=20$, $b=0.2$ y $c=2\\pi$\n",
    "<br>![image](imagenes/Ackley.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra el funcionamiento del algoritmo SA para encontrar el mínimo de la funcion Ackley a través de una imagen interactiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>![SegmentLocal](../gifs/ackley.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema del Agente Viajero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema del agente viajero, también conocido como TSP por sus siglas en inglés, es un problema bastante conocido que permite a la gente poner a prueba sus algoritmos de optimización. Este probelma es usado para minimizar una función que representa la distancia que una persona tendría que viajar, por lo que se tomó la decisión que implementar el algoritmo de SA en este problema buscando encontrar un punto mínimo global para la función de distancia objetivo. \n",
    "\n",
    "TSP es la abstracción de un problema que consiste en minimizar la distancia que un vendedor debería recorrer al visitar un conjunto de ciudades representadas como un par de coordenadas $(x_{i}, y_{i}$ en un plano de dos dimensiones. Como se mencionó anteriormente, se busca que el vendedor pueda recorrer las $N$ ciudades partiendo de una ciudad inicial, sin embargo se tiene la restricción de que se puede visitar cada ciudad únicamente una vez, y al finalizar el recorrido debe regresar a la ciudad en la que se inició, por lo que la ruta que debiera seguir el vendedor debe ser la más corta (Press & Al, 2007).\n",
    "\n",
    "Lo interesante de este problema recae en la cantidad de ciudades que se deben visitar, pues conforme $N$ crece, la complejidad computacional es mayor. Dado que existen múltiples soluciones a este problema que pueden interpretarse como mínimos locales, lo ideal es encontrar la distancia mínima que debiera recorrerse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated annealing y TSP\n",
    "\n",
    "Como es de esperarse, SA resulta un buen algoritmo para atacar problemas como TSP que se componen de múltiples mínimos locales, por lo que se tiene como objetivo resolver este problema tomando ventaja del algoritmo SA desarrollado previamente. \n",
    "\n",
    "Para nuestro propósito, la definición de cada una de las ciudades se hizo de forma pseudo-aleatoria usando librerías del lenguaje de programación Python. Así mismo, el problema fue acotado al espacio unitario, es decir que todas las ciudades se encuentran dentro de un plano $(x,y)$ donde el valor mínimo de las coordenadas es $x=0$ y $y=0$ y el valor máximo puede ser $x=1$ y $y=1$.\n",
    "\n",
    "Una consideración adicional dentro de la solución de este problema es la permutación del movimiento del viajero entre ciudades vecinas. Para llevar a cabo esta consideración, se evalúa la ruta que sigue el viajero dado un orden de ciudades, y posteriormente se procede a realizar una permutación del orden que siguen estas mismas ciudades para generar diferentes soluciones y encontrar la mínima distancia. \n",
    "\n",
    "Es importante mencionar que este proceso de permutaciones y búsqueda del mínimo se realiza una cantidad de veces predefinida, y que para incluir la disminución de temperatura que establece el algoritmo de SA es necesario replicar el algoritmo una cantidad de iteraciones también predefinida. De esta manera, la complejidad del algoritmo se vuelve $num_iter * num_permutaciones*, sin embargo la cantidad de permutaciones ideales depende de la cantidad de ciudades con las que se cuente en el problema, por lo que son parametros que se requieren optimizar mediante un proceso de validación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen muestra el estado inicial de nuestro problema para una cantidad de 20 ciudades, donde puede observarse que la ruta establecida entre cada una de ellas no es la ideal. \n",
    "<br>![image](imagenes/Inicial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente malla de figuras representa la convergencia del algoritmo de SA en el problema del agente viajero, donde puede observarse la minimización de la distancia que debiera recorrer el agente entre ciudades bajo las restricciones presentadas anteriormente. Los puntos verde y rojo representan las ciudades donde el viajero inicia el recorrido y donde lo termina, mientras que los puntos morados son las ciudades que se recorren en el camino. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](imagenes/Malla.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, se incluye una imagen interactiva donde se muestra el proceso de convergencia para un problema TSP usando SA para la minimización de la distancia. \n",
    "\n",
    "<br>![SegmentLocal](../gifs/tsp.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "* Una vez que el equipo completo entendió el funcionamiento y aplicaciones que tiene el método SA, fue posible comenzar a trabajar en replicar su funcionalidad a través de código de `Python`.\n",
    "* Comparado con otros métodos, por ejemplo colinas ascendentes, el concepto de probabilidad de aceptación provoca que SA sea bueno para encontrar mínimos globales sin depender de la exhaustividad de la búsqueda, lo cual también lo hace eficiente.\n",
    "* Aplicando el SA que se programó en `Python` a las funciones propuestas por el equiSe comprobó que SA, efectivamente, es un método robusto ante la presencia de mínimos locales en problemas de optimización.\n",
    "* Adicionalmente, identificamos que una de las vulnerabilidades del método se presenta cuando el descenso en la temperatura es muy rápido. Bajo esta circunstancia, se corre el riesgo de no alcanzar el mínimo global.\n",
    "* Se pudo aproximar el mínimo de las funciones propuestas con el método SA.\n",
    "* Con relación al problema del agente viajero, a lo largo del trabajo observamos que SA permite encontrar con relativa facilidad la ruta de menor distancia en este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias \n",
    "- Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by Simulated Annealing. Science, 220(4598), 671–680. https://doi.org/10.1126/science.220.4598.671\n",
    "\n",
    "- Kochnderfer, M. J., & Wheeler, T. A. (2019). Algorithms for optimization. The Mit Press, 128-133.\n",
    "\n",
    "- Liang, F. (2020, April 21). Optimization Techniques — Simulated Annealing - Towards Data Science. Medium; Towards Data Science. https://towardsdatascience.com/optimization-techniques-simulated-annealing-d6a4785a1de7\n",
    "\n",
    "- Press, W. H., & Al, E. (2007). Numerical recipes : the art of scientific computing (3rd ed.). Cambridge University Press.\n",
    "\n",
    "- Surjanovic, S. & Bingham, D. (2013). Virtual Library of Simulation Experiments: Test Functions and Datasets. Retrieved December 13, 2022, from http://www.sfu.ca/~ssurjano.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
